{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/languages\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/sentiment\n"
     ]
    }
   ],
   "source": [
    "subscription_key = 'e41c34b3bcf841d0859d6aba9e8e1494'\n",
    "assert subscription_key\n",
    "text_analytics_base_url = \"https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/\"\n",
    "\n",
    "language_api_url = text_analytics_base_url + \"languages\"\n",
    "print(language_api_url)\n",
    "\n",
    "sentiment_api_url = text_analytics_base_url + \"sentiment\"\n",
    "print(sentiment_api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to return tweet language and sentiment score\n",
    "\n",
    "def language_and_sentiment(df):\n",
    "    \n",
    "    #Language\n",
    "    id_count = 0\n",
    "    documents = {'documents': []}\n",
    "    \n",
    "    #compile language document\n",
    "    for text in df['clean_text']:        \n",
    "        documents['documents'].append({'id': '{}'.format(id_count), 'text': text})\n",
    "        id_count += 1\n",
    "        \n",
    "    #feed compiled doc to API    \n",
    "    headers   = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
    "    response  = requests.post(language_api_url, headers=headers, json=documents)\n",
    "    languages = response.json()\n",
    "    \n",
    "    new_lang_dict = {}\n",
    "    for entry in languages['documents']:\n",
    "        new_lang_dict[entry['id']] = entry['detectedLanguages'][0]['iso6391Name']\n",
    "    \n",
    "    \n",
    "    langs = []\n",
    "    for i in range(0,len(df)):\n",
    "        if str(i) in new_lang_dict.keys():\n",
    "            langs.append(new_lang_dict[str(i)])\n",
    "        else:\n",
    "            langs.append(np.nan)\n",
    "        \n",
    "    \n",
    "\n",
    "    #append languages to df\n",
    "    df['language'] = langs\n",
    "    \n",
    "    \n",
    "    ## Sentiment\n",
    "    id_count = 0\n",
    "    documents = {'documents': []}\n",
    "    \n",
    "    #compile sentiment document\n",
    "    for text in df['clean_text']:\n",
    "        documents['documents'].append({'id': '{}'.format(id_count), 'language': langs[id_count], 'text': text})\n",
    "        id_count += 1\n",
    "        \n",
    "    #feed compiled sentiment doc to API\n",
    "    headers   = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
    "    response  = requests.post(sentiment_api_url, headers=headers, json=documents)\n",
    "    sentiments = response.json()\n",
    "    \n",
    "    new_sent_dict = {}\n",
    "    for entry in sentiments['documents']:\n",
    "        new_sent_dict[entry['id']] = entry['score']\n",
    "    \n",
    "    \n",
    "    sents = []\n",
    "   \n",
    "    for i in range(0,len(df)):\n",
    "        if str(i) in new_sent_dict.keys():\n",
    "            sents.append(new_sent_dict[str(i)])\n",
    "        else:\n",
    "            sents.append(np.nan)\n",
    "        \n",
    "        \n",
    "    \n",
    "    df['sentiment'] = sents\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean tweet text\n",
    "\n",
    "def clean_tweet(row):\n",
    "    text = re.sub('Retweeted.*\\):','',row['text'])\n",
    "    text = re.sub('(http.*$)','',text)\n",
    "    text = re.sub('(pic.twitter.*$)','',text)\n",
    "    text = re.sub('([@#][\\w_-]+)','',text)\n",
    "    text = re.sub('([@] [^ ]+)','', text)\n",
    "    text = re.sub('([#] [^ ]+)','', text)\n",
    "    text = re.sub('[^\\w]', ' ', text)\n",
    "    text = re.sub('(['  '.*])', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size].copy() for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/languages\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/hackathon/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/languages\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/sentiment\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/languages\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/sentiment\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/languages\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/sentiment\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/languages\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 4073: expected 10 fields, saw 12\\n'\n",
      "b'Skipping line 14044: expected 10 fields, saw 11\\n'\n",
      "b'Skipping line 30941: expected 10 fields, saw 11\\nSkipping line 33717: expected 10 fields, saw 12\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/languages\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/sentiment\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/languages\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 17709: expected 10 fields, saw 11\\nSkipping line 18724: expected 10 fields, saw 11\\n'\n",
      "b'Skipping line 24832: expected 10 fields, saw 11\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/languages\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 14699: expected 10 fields, saw 12\\nSkipping line 15499: expected 10 fields, saw 12\\n'\n",
      "b'Skipping line 28221: expected 10 fields, saw 11\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/languages\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/sentiment\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/languages\n",
      "https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6353: expected 10 fields, saw 11\\n'\n"
     ]
    }
   ],
   "source": [
    "#run through csvs in the directory, process and save in new folder\n",
    "\n",
    "open_directory = pathlib.Path('data/clean')\n",
    "save_path = 'data/sentiment'\n",
    "\n",
    "for csv in open_directory.glob('*.csv'):\n",
    "         \n",
    "    all_tweets = []\n",
    "    for tweets in pd.read_csv(csv, sep=';', chunksize = 10000,iterator=True, low_memory=False, error_bad_lines = False):\n",
    "\n",
    "    #tweets = pd.read_csv(csv, sep=';', error_bad_lines = False)\n",
    "    \n",
    "    \n",
    "        #binary indicator of whether the tweet was a retweet\n",
    "        tweets['retweet'] = tweets['text'].apply(lambda x: re.sub(r'Retweeted.*\\):','',x))\n",
    "    \n",
    "        #clean tweet text\n",
    "        tweets['clean_text'] = tweets.apply(clean_tweet, axis=1)\n",
    "        tweets = tweets[tweets['clean_text'].str.contains('([a-zA-Z]+)', regex=True)]\n",
    "    \n",
    "        chunk_dfs = []\n",
    "    \n",
    "        for i in chunker(tweets,500):\n",
    "            time.sleep(10)\n",
    "            chunk_dfs.append(language_and_sentiment(i))\n",
    "    \n",
    "        tweets_with_sent = pd.concat(chunk_dfs)\n",
    "        all_tweets.append(tweets_with_sent)\n",
    "    \n",
    "    \n",
    "    final = pd.concat(all_tweets)\n",
    "    save_file = (save_path + str(csv)[10:-7] + 'sent.csv')\n",
    "    \n",
    "    final.to_csv(save_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
