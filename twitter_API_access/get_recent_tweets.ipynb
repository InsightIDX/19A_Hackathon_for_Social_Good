{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import tweepy\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = '#1May'         # <----- user input\n",
    "time_period = 0.5 # hours     # <----- user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = '867046056-cvYtXPfzYSnftVl8wwExAc9LDm4XmF0CcOfJ7fZw'\n",
    "ACCESS_SECRET = 'MMLxDnE4dNO8BXQQ1RfelhJEi6n8S82H1DPXXeFlShQH7'\n",
    "CONSUMER_KEY = 'o2FVKVmYuOscIvDOgBoLwQdg2'\n",
    "CONSUMER_SECRET = 'jzbeDlSb5cJbfJEXERbR47JjaLpzC85u9nrquiMy4Ex7aFL9bD'\n",
    "\n",
    "# Setup tweepy to authenticate with Twitter credentials:\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "# Create the api to connect to twitter with your creadentials\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate start time of search (current time - time_period)\n",
    "\n",
    "current_time = pd.Timestamp.now(tz='UTC')\n",
    "start_time = current_time - datetime.timedelta(hours = time_period)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file with records of prev searches\n",
    "\n",
    "search_params = pd.read_csv('search_param.txt',\n",
    "                            infer_datetime_format = True,\n",
    "                            parse_dates = ['start_time', 'end_time'])\n",
    "\n",
    "search_params.sort_values(by = 'end_time', ascending = False, inplace = True)\n",
    "search_params.index = range(len(search_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing data\n"
     ]
    }
   ],
   "source": [
    "# check if query in search_params, if start_time > START_TIME and start_time < END_TIME\n",
    "# if not, then search normally\n",
    "\n",
    "# if yes, load file and search with last_id\n",
    "\n",
    "found_flag = False\n",
    "end_id = 0\n",
    "\n",
    "i = 0\n",
    "while i < len(search_params):\n",
    "    \n",
    "    if ((search_params.loc[i]['query'] == query) &\n",
    "        (search_params.loc[i]['start_time'] <= start_time) &\n",
    "        (search_params.loc[i]['end_time'] > start_time)):\n",
    "        \n",
    "        found_flag = True\n",
    "        end_id = search_params.loc[i]['end_id']\n",
    "        filename = search_params.loc[i]['filename']\n",
    "        \n",
    "        print('Found existing data')\n",
    "        i = 9999\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "if found_flag:    \n",
    "    json_tweets_old = pd.read_csv(filename,\n",
    "                                  infer_datetime_format = True,\n",
    "                                  parse_dates = ['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweets = 20000\n",
    "query = search_term + ' -filter:retweets'\n",
    "\n",
    "searched_tweets = []\n",
    "last_searched_id = -1\n",
    "since_id = end_id\n",
    "\n",
    "while len(searched_tweets) < max_tweets:\n",
    "    count = max_tweets - len(searched_tweets)\n",
    "    try:\n",
    "        \n",
    "        new_tweets = api.search(q=query,\n",
    "                                count = count,\n",
    "                                max_id = str(last_searched_id - 1),\n",
    "                                since_id = since_id,\n",
    "                                tweet_mode = 'extended',\n",
    "                                since = str(start_time.date()),\n",
    "                                lang = 'es')\n",
    "        \n",
    "        \n",
    "        if not new_tweets:\n",
    "            break\n",
    "            \n",
    "            \n",
    "            \n",
    "        searched_tweets.extend(new_tweets)\n",
    "        last_searched_id = new_tweets[-1].id\n",
    "        \n",
    "        if pd.to_datetime(new_tweets[-1]._json['created_at']) < start_time:\n",
    "            print('Reached start time')\n",
    "            break\n",
    "        \n",
    "        \n",
    "    except tweepy.TweepError as e:\n",
    "        # depending on TweepError.code, one may want to retry or wait\n",
    "        # to keep things simple, we will give up on an error\n",
    "        break\n",
    "\n",
    "        \n",
    "json_tweets = [i._json for i in searched_tweets]\n",
    "json_tweets_pd = pd.DataFrame(json_tweets)\n",
    "json_tweets_pd['datetime'] = pd.to_datetime(json_tweets_pd['created_at'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "if found_flag:\n",
    "    # append the old data below the new data\n",
    "    \n",
    "    json_tweets_pd = json_tweets_pd.append(json_tweets_old,  ignore_index = True, sort = True)\n",
    "\n",
    "    \n",
    "    \n",
    "# save data, update and save search params file\n",
    "\n",
    "time_start = json_tweets_pd['datetime'][len(json_tweets_pd)-1]\n",
    "time_end = json_tweets_pd['datetime'][0]\n",
    "\n",
    "end_id = json_tweets_pd['id'].max()\n",
    "filename = 'tweet_files/search_results_' + time_end.isoformat().replace(':','_') + '.txt'\n",
    "\n",
    "\n",
    "json_tweets_pd.to_csv(filename)\n",
    "\n",
    "\n",
    "last_ind = len(search_params)\n",
    "search_params.loc[last_ind] = [query, time_start, time_end, end_id, filename]\n",
    "\n",
    "search_params.to_csv('search_param.txt', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
